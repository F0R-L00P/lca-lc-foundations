{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": sys.executable,\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d917d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a helpful assistant that answers user questions about LangChain, LangGraph and LangSmith.\\n\\n    You can use the following tools/resources to answer user questions:\\n    - search_web: Search the web for information\\n    - github_file: Access the langchain-ai repo files\\n\\n    If the user asks a question that is not related to LangChain, LangGraph or LangSmith, you should say \"I\\'m sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"\\n\\n    You may try multiple tool and resource calls to answer the user\\'s question.\\n\\n    You may also ask clarifying questions to the user to better understand their question.\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='e3116f0c-16ca-479a-83a5-5560f85dbdb2'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 271, 'total_tokens': 363, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpTiwkMZhMcnajrgO4hlRakaBYhHq', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b44c0-8252-7370-b185-d29636f95a55-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_MWq9JkVis493S1n9qhtIvvKN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 92, 'total_tokens': 363, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9325087,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.\",\\n      \"score\": 0.90791017,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0\",\\n      \"title\": \"LangChain MCP Adapters 0.2.0\",\\n      \"content\": \"LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings quality-of-life upgrades for anyone building with MCP tools in LangChain:. Image 2**Multimodal tool support**. Use tools that accept and produce images, text, and other modalitiesâ€”powered by LangChainâ€™s standard content blocks. Image 3**Elicitation support via callbacks**. Easily implement prompt-driven clarifications and multi-turn tool interactions. Image 4**Structured tool output as artifacts**. Tool results now store structured content as artifacts, making downstream processing and inspection much cleaner. Image 5**Tool name prefixes for multi-server setups**. Eliminate naming collisions and run multiple MCP servers seamlessly. We\\'ve also released new docs to help you get started fast: https://docs.langchain.com/oss/python/langchain/mcp. Technical release notes: https://github.com/langchain-ai/langchain-mcp-adapters/releases/tag/langchain-mcp-adapters%3D%3D0.2.0. ##### Subscribe to updates. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply.\",\\n      \"score\": 0.86928266,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.84858394,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"streamable_http\\\\\", // HTTP-based remote server transport:  \\\\\"streamable_http\\\\\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.82895297,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"771a33d9-c879-4b14-880d-b3d76cb64793\"\\n}', 'id': 'lc_a679fa52-e24d-475f-8792-ccc952b0d5e6'}], name='search_web', id='12563e00-900d-4f79-9e48-5023a00b74bb', tool_call_id='call_MWq9JkVis493S1n9qhtIvvKN', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9325087, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': '[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.', 'score': 0.90791017, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0', 'title': 'LangChain MCP Adapters 0.2.0', 'content': \"LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings quality-of-life upgrades for anyone building with MCP tools in LangChain:. Image 2**Multimodal tool support**. Use tools that accept and produce images, text, and other modalitiesâ€”powered by LangChainâ€™s standard content blocks. Image 3**Elicitation support via callbacks**. Easily implement prompt-driven clarifications and multi-turn tool interactions. Image 4**Structured tool output as artifacts**. Tool results now store structured content as artifacts, making downstream processing and inspection much cleaner. Image 5**Tool name prefixes for multi-server setups**. Eliminate naming collisions and run multiple MCP servers seamlessly. We've also released new docs to help you get started fast: https://docs.langchain.com/oss/python/langchain/mcp. Technical release notes: https://github.com/langchain-ai/langchain-mcp-adapters/releases/tag/langchain-mcp-adapters%3D%3D0.2.0. ##### Subscribe to updates. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply.\", 'score': 0.86928266, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.84858394, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { ChatAnthropic } from \"@langchain/anthropic\"; import { ChatAnthropic } from \"@langchain/anthropic\";import { createAgent } from \"langchain\"; import { createAgent } from  \"langchain\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \"stdio\", // Local subprocess communication transport:  \"stdio\", // Local subprocess communication command: \"node\", command:  \"node\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\"/path/to/math_server.js\"], args: [\"/path/to/math_server.js\"], }, }, weather: { weather: { transport: \"streamable_http\", // HTTP-based remote server transport:  \"streamable_http\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \"http://localhost:8000/mcp\", url: \"http://localhost:8000/mcp\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.', 'score': 0.82895297, 'raw_content': None}], 'response_time': 0.0, 'request_id': '771a33d9-c879-4b14-880d-b3d76cb64793'}}}),\n",
      "              AIMessage(content='Hereâ€™s a concise overview of the langchain-mcp-adapters library and how it fits into LangChain/LangGraph and MCP.\\n\\nWhat it is\\n- The langchain-mcp-adapters library is a bridge that lets LangChain and LangGraph agents use tools defined under the Model Context Protocol (MCP) across one or more MCP servers.\\n- It converts MCP tools into LangChain- and LangGraph-compatible tools, enabling agents to interact with many tool servers in a unified way.\\n\\nWhat MCP is (brief)\\n- MCP (Model Context Protocol) is an open standard for how applications provide tools and context to LLMs.\\n- MCP servers expose tools; the adapters translate those tools into a form usable by LangChain/LangGraph agents.\\n- MCP tools can return structured content and multimodal results, and sessions can be stateful across tool calls.\\n\\nKey features (based on the MCP Adapters releases)\\n- Cross-server tool access: Agents can pull tools from multiple MCP servers and use them together.\\n- Easy integration with MCP tool ecosystems: No need to manually adapt each MCP tool for LangChain/LangGraph.\\n- Multimodal tool support (from recent releases): Tools can handle images, text, and other modalities.\\n- Elicitation support via callbacks: Easy for prompts to request additional user input during tool execution.\\n- Structured outputs as artifacts: Tool results can be captured as structured artifacts for easier downstream use.\\n- Tool name prefixes for multi-server setups: Helps avoid naming collisions when connecting to many MCP servers.\\n- Documentation and examples for Python and JavaScript: Covers how to initialize, fetch tools, and build agents with MCP tools.\\n\\nWhere to find it\\n- GitHub (Python package): langchain-ai/langchain-mcp-adapters\\n  - https://github.com/langchain-ai/langchain-mcp-adapters\\n- LangChain docs (Python): Model Context Protocol (MCP) and adapters\\n  - https://docs.langchain.com/oss/python/langchain/mcp\\n- LangChain docs (JavaScript): MCP adapters in JS/TS\\n  - https://docs.langchain.com/oss/javascript/langchain/mcp\\n- LangChain changelog entries:\\n  - MCP Adapters for LangChain and LangGraph (overview)\\n    - https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\\n  - LangChain MCP Adapters 0.2.0 (new features and docs)\\n    - https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0\\n\\nHow to use (high level)\\n- Python\\n  - Create a MultiServerMCPClient with your MCP servers configured (one per server with its transport/connection details).\\n  - Use client.get_tools() to fetch the available MCP tools.\\n  - Pass those tools to a LangChain/LangGraph agent (e.g., via create_agent) so the agent can solve tasks using MCP tools from multiple servers.\\n  - If you need lifecycle management for sessions, you can use a persistent ClientSession (client.session()).\\n- JavaScript/TypeScript\\n  - Create a MultiServerMCPClient with server specs.\\n  - Retrieve tools and wire them into a LangChain agent similarly to the Python flow.\\n  - The JS docs provide equivalent patterns for initializing the client, getting tools, and creating agents.\\n\\nWhy you might use it\\n- You already have MCP servers or want to participate in the MCP ecosystem, and you want to leverage LangChain/LangGraph agents to orchestrate tools across servers.\\n- You want to combine tools from different MCP servers into a single agent workflow without writing custom adapters for each tool.\\n- You benefit from MCP features like structured outputs, elicitation, and multimodal results within LangChain/LangGraph workflows.\\n\\nWould you like a quickstart example (Python or JavaScript) or help picking MCP server configurations for your use case? I can provide a minimal code snippet to get you started, or point you to the exact docs for your preferred language.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2044, 'prompt_tokens': 2322, 'total_tokens': 4366, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpTj0TRxkUWrlSjmQAUhcmX1HQcR7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b44c0-8fe9-7cb3-9fe0-e3b6cb425596-0', usage_metadata={'input_tokens': 2322, 'output_tokens': 2044, 'total_tokens': 4366, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/Denver\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='7d44efa7-b569-4379-acc5-cc2364a8d285'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 282, 'prompt_tokens': 293, 'total_tokens': 575, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpTlCbNy2Vf1Sc6XS424pgRzLKHaW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b44c2-a105-7813-9bfe-a34fe1eb6fc9-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/Denver'}, 'id': 'call_uuOWp5bV1UC1IFST5arvXcaf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 293, 'output_tokens': 282, 'total_tokens': 575, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/Denver\",\\n  \"datetime\": \"2025-12-21T23:32:49-07:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_90bf26e7-7cf0-4761-b28a-150e3467d928'}], name='get_current_time', id='9d5899b8-c628-4021-973b-85bd904565ad', tool_call_id='call_uuOWp5bV1UC1IFST5arvXcaf'),\n",
      "              AIMessage(content=\"Right now in Denver (America/Denver) it's 23:32:49 on Sunday, December 21, 2025 (UTC-07:00). Want this in another timezone?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 560, 'prompt_tokens': 374, 'total_tokens': 934, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpTlGpRBQXpVvdojzHV0K9HgJGJ2u', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b44c2-b3ef-7431-8f1e-98b64f362037-0', usage_metadata={'input_tokens': 374, 'output_tokens': 560, 'total_tokens': 934, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-foundations (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

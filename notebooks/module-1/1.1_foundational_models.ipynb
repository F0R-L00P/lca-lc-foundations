{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106cf38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e9c4a",
   "metadata": {},
   "source": [
    "## Initialising and invoking a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf169b71",
   "metadata": {},
   "source": [
    "## Customising your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f50c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    # Kwargs passed to the model:\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20521bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during chat completion creation. (POST https://us-south.ml.cloud.ibm.com/ml/gateway/v1/chat/completions)\n",
      "Status code: 404, body: <html>\n",
      "<head><title>404 Not Found</title></head>\n",
      "<body>\n",
      "<center><h1>404 Not Found</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "You are calling the Model Gateway endpoint using the 'model' parameter. Please ensure this model is registered with the Gateway. If you intend to use a watsonx.ai-hosted model, pass 'model_id' instead of 'model'.\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during chat completion creation. (POST https://us-south.ml.cloud.ibm.com/ml/gateway/v1/chat/completions)\nStatus code: 404, body: <html>\r\n<head><title>404 Not Found</title></head>\r\n<body>\r\n<center><h1>404 Not Found</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mApiRequestFailure\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ibm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatWatsonx\n\u001b[32m      3\u001b[39m model = ChatWatsonx(\n\u001b[32m      4\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mibm/granite-34b-code-instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Kwargs passed to the model:\u001b[39;00m\n\u001b[32m      6\u001b[39m     temperature=\u001b[32m1.0\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the capital of the Moon?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_ibm\\chat_models.py:1252\u001b[39m, in \u001b[36mChatWatsonx._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.watsonx_model_gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1251\u001b[39m     call_kwargs = {**kwargs, **updated_params}\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_model_gateway\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1258\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.watsonx_model.chat(\n\u001b[32m   1259\u001b[39m         messages=message_dicts,\n\u001b[32m   1260\u001b[39m         **(kwargs | {\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: updated_params}),\n\u001b[32m   1261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_ibm\\utils.py:93\u001b[39m, in \u001b[36mgateway_error_handler.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ApiRequestFailure:\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m     96\u001b[39m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n\u001b[32m     97\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mwatsonx_model_gateway\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwatsonx_embed_gateway\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\langchain_ibm\\chat_models.py:1212\u001b[39m, in \u001b[36mChatWatsonx._call_model_gateway\u001b[39m\u001b[34m(self, model, messages, **params)\u001b[39m\n\u001b[32m   1208\u001b[39m \u001b[38;5;129m@gateway_error_handler\u001b[39m\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_model_gateway\u001b[39m(\n\u001b[32m   1210\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, model: \u001b[38;5;28mstr\u001b[39m, messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], **params: Any\n\u001b[32m   1211\u001b[39m ) -> Any:\n\u001b[32m-> \u001b[39m\u001b[32m1212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwatsonx_model_gateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\ibm_watsonx_ai\\gateway\\gateway.py:187\u001b[39m, in \u001b[36mGateway.__init__.<locals>._ChatCompletions.create\u001b[39m\u001b[34m(self, model, messages, stream, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _streaming_create(\n\u001b[32m    178\u001b[39m         api_client=\u001b[38;5;28mself\u001b[39m._client, url=url, request_json=request_json\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    181\u001b[39m response = \u001b[38;5;28mself\u001b[39m._client.httpx_client.post(\n\u001b[32m    182\u001b[39m     url=url,\n\u001b[32m    183\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m._client._get_headers(),\n\u001b[32m    184\u001b[39m     json=request_json,\n\u001b[32m    185\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat completion creation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\behna\\Documents\\lca-lc-foundations\\.venv\\Lib\\site-packages\\ibm_watsonx_ai\\wml_resource.py:180\u001b[39m, in \u001b[36mWMLResource._handle_response\u001b[39m\u001b[34m(cls, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ApiRequestFailure(\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailure during \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperationName\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    182\u001b[39m         response,\n\u001b[32m    183\u001b[39m     )\n",
      "\u001b[31mApiRequestFailure\u001b[39m: Failure during chat completion creation. (POST https://us-south.ml.cloud.ibm.com/ml/gateway/v1/chat/completions)\nStatus code: 404, body: <html>\r\n<head><title>404 Not Found</title></head>\r\n<body>\r\n<center><h1>404 Not Found</h1></center>\r\n<hr><center>nginx</center>\r\n</body>\r\n</html>\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import ChatWatsonx\n",
    "\n",
    "model = ChatWatsonx(\n",
    "    model_id=\"ibm/granite-34b-code-instruct\",\n",
    "    url=os.environ[\"WATSONX_URL\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14f450",
   "metadata": {},
   "source": [
    "## Model Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf1d9f",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/integrations/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db029be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model=\"claude-sonnet-4-5\")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc723f48",
   "metadata": {},
   "source": [
    "## Initialising and invoking an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ef4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(model=\"claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5251725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac95763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5da573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\"),\n",
    "    AIMessage(content=\"The capital of the Moon is Luna City.\"),\n",
    "    HumanMessage(content=\"Interesting, tell me more about Luna City\")]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf635e",
   "metadata": {},
   "source": [
    "## Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me all about Luna City, the capital of the Moon\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "\n",
    "    # token is a message chunk with token content\n",
    "    # metadata contains which node produced the token\n",
    "    \n",
    "    if token.content:  # Check if there's actual content\n",
    "        print(token.content, end=\"\", flush=True)  # Print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef691a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-foundations (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
